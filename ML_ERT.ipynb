{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import importlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "import sklearn\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier,AdaBoostClassifier\n",
    "import scikitplot as skplt\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "from sklearn.inspection import partial_dependence\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "from pdpbox import pdp, info_plots\n",
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "from pdpbox import pdp_plot_utils\n",
    "from pptx import Presentation\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "from pptx.util import Inches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read, Warngle and Tranform Data for ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first read#\n",
    "data=pd.read_csv('raw_2500.csv',encoding='utf-8-sig')\n",
    "#data.rename(columns=dict(zip(data.columns.tolist(),['feat'+i for i in np.arange(1,len(data.columns)+1).astype(str)])),inplace=True)\n",
    "#date#\n",
    "tarikh=pd.read_csv('183104-arash.csv',encoding='utf-8-sig')\n",
    "date=tarikh.iloc[:,[2,3,5,6,7,8,9,10,20]]\n",
    "date['VAR3']=date['VAR3'].astype('datetime64[ns]').dt.date\n",
    "date.drop_duplicates(inplace=True)\n",
    "#x_vars=pd.read_csv('selected_varbs1.csv',encoding='utf-8-sig')\n",
    "x_vars=pd.read_csv('selected_varbs2.csv',encoding='utf-8-sig')\n",
    "\n",
    "data['dep_var']=1*data.iloc[:,22]+2*data.iloc[:,21]+3*data.iloc[:,23]\n",
    "df=data.loc[:,x_vars.definition]\n",
    "df['Start_GDate/Time']=data['Start_GDate/Time']\n",
    "df['dep_var'] = data['dep_var']\n",
    "df.dropna(inplace=True)\n",
    "#merge df and date#\n",
    "df['Start_GDate/Time']=df['Start_GDate/Time'].astype('datetime64[ns]').dt.date\n",
    "df=df.merge(date,how='left',left_on='Start_GDate/Time',right_on='VAR3')\n",
    "df.dropna(inplace=True)\n",
    "df=df.loc[df['dep_var']!=0]\n",
    "\n",
    "qwe=['HourlyTrafficDataset_TotalCarFlowRates(Veh/min)_mean',\n",
    "     'HourlyTrafficDataset_HeadWay(sec)_mean','HourlyTrafficDataset_TotalAverageSpeed(km/h)_mean']\n",
    "ffr=['Flowrate','Headway','Speed']\n",
    "\n",
    "for i,j in zip(qwe,ffr):\n",
    "    bins = [0*df[i].max(), 0.2*df[i].max(), 0.4*df[i].max(), 0.6*df[i].max(), 0.8*df[i].max(), 1*df[i].max()]\n",
    "    group_names = [1,2,3,4,5]\n",
    "    \n",
    "    df[j]=pd.cut(df[i], bins, labels=group_names)\n",
    "    \n",
    "trf_cor=[]\n",
    "for i in ffr:\n",
    "    s=abs(pd.concat([df[['dep_var']],pd.get_dummies(df[i],prefix=i+'_')]\n",
    "                    ,axis=1).corr()[['dep_var']]).sort_values(by='dep_var',ascending=False).index[1]   \n",
    "    trf_cor.append(s)\n",
    "    df=pd.concat([df,pd.get_dummies(df[i],prefix=i+'_')],axis=1)\n",
    "#create dummies of calendar varbs \n",
    "yu={}\n",
    "for t in tarikh.iloc[:,[5,6,7,8,9,10]].columns:\n",
    "    if t==\"VAR11\":\n",
    "        yu[t]=pd.get_dummies(df[t],drop_first=True).add_suffix('th year')\n",
    "    else:\n",
    "        yu[t]=pd.get_dummies(df[t],drop_first=True) \n",
    "added=pd.concat(yu,axis=1)\n",
    "added.columns=[added.columns[i][1] for i in range(len(added.columns))]\n",
    "df=pd.concat([df,added],axis=1)\n",
    "\n",
    "df['mot_cycl']=np.where((df.loc[:,[x for x in df.columns if \"موتورسیکلت\" in x]].sum(axis=1))>0,1,0)\n",
    "df['car']=np.where((df.loc[:,[x for x in df.columns if \"سواری\" in x]].sum(axis=1))>0,1,0)\n",
    "\n",
    "y = df['dep_var']\n",
    "c=abs(df.corr()[['dep_var']]).sort_values(by='dep_var',ascending=False).head(n=34).index.tolist()\n",
    "c.remove('dep_var')\n",
    "c.remove('فوریه')\n",
    "c.remove('VAR11')\n",
    "c.remove('4.0th year')\n",
    "#c.remove('CrashDataset_Lighting_Condition_شب بدون روشنایی کافی')\n",
    "#c.remove('CrashDataset_Vehicle_1_Group_اتوبوس')\n",
    "#c.remove('CrashDataset_Roadway_Surface_Condition_یخبندان و برفی')\n",
    "\n",
    "#c.remove('رجب')\n",
    "\n",
    "#c.remove('CrashDataset_Vehicle_12_Group_سواری')\n",
    "#c.remove('CrashDataset_Vehicle_12_Group_احشام')\n",
    "c.remove('CrashDataset_Vehicle_12_Group_موتورسیکلت')\n",
    "c.remove('CrashDataset_Vehicle_1_Group_موتورسیکلت')\n",
    "#c.remove('CrashDataset_Vehicle_1_Group_سواری')\n",
    "ffr=['Flowrate','Headway','Speed']\n",
    "\n",
    "c=[x for x in c if \"HourlyTrafficDataset\" not in x]\n",
    "ert=c+qwe\n",
    "ert.remove('Speed__1')\n",
    "x = df.loc[:,ert]\n",
    "#names=pd.read_csv('check.csv',encoding='utf-8-sig')\n",
    "#x.columns=names['def2'][:-1]\n",
    "#x.drop(columns=['CrashDataset_Vehicle_1_Action_حرکت به جلو'],inplace=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a GridSearch for algorithms who are exist in SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "#ss=['CrashDataset_Vehicle_1_Action_سبقت','CrashDataset_Vehicle_1_Action_گردش به چپ']\n",
    "#param_grid = {'n_estimators': [15,35,70,141,210],\n",
    "#              'max_depth':np.arange(5,25,2),\n",
    "#              'max_features':('auto','log2'),\n",
    "#              'min_samples_split': (np.arange(0.01,0.21,0.02)*len(x_train)).astype(int)}\n",
    "param_grid = {'n_estimators': list(np.arange(1,500,4).astype(int)),\n",
    "             'algorithm':['SAMME', 'SAMME.R']} #param for adaboost\n",
    "\n",
    "grid = GridSearchCV(AdaBoostClassifier(),param_grid, cv=5, verbose=50,refit = True,n_jobs=-1) \n",
    "grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ERT=ExtraTreesClassifier(max_depth= 13,\n",
    "#                         max_features= 'auto',\n",
    "#                               min_samples_split= 11,\n",
    "#                               n_estimators= 141)\n",
    "\n",
    "#ERT=ExtraTreesClassifier(max_depth= 9,\n",
    "# max_features= 'auto',\n",
    "# min_samples_split= 79,\n",
    "# n_estimators= 35)\n",
    "#ERT.fit(x_train,y_train)\n",
    "#y_train_pred = ERT.predict(x_train)\n",
    "#y_test_pred = ERT.predict(x_test)\n",
    "\n",
    "\n",
    "ADAB=AdaBoostClassifier(n_estimators= 73,algorithm='SAMME')\n",
    "ADAB.fit(x_train,y_train)\n",
    "y_train_pred = ADAB.predict(x_train)\n",
    "y_test_pred = ADAB.predict(x_test)\n",
    "\n",
    "print('accuracy train:  ' + str(sklearn.metrics.accuracy_score(y_train, y_train_pred)))\n",
    "print('cf_matrix on train:    ' + str(sklearn.metrics.confusion_matrix(y_train, y_train_pred)))\n",
    "\n",
    "print('accuracy test:   ' + str(sklearn.metrics.accuracy_score(y_test, y_test_pred)))\n",
    "print('cf_matrix: on test:     ' + str(sklearn.metrics.confusion_matrix(y_test, y_test_pred)))\n",
    "\n",
    "print('precision_score train:  ' + str(sklearn.metrics.precision_score(y_train, y_train_pred, average=\"weighted\")))\n",
    "print('precision_score test:   ' + str(sklearn.metrics.precision_score(y_test, y_test_pred, average=\"weighted\")))\n",
    "\n",
    "print('recall_score train:  ' + str(sklearn.metrics.recall_score(y_train, y_train_pred, average=\"weighted\")))\n",
    "print('recall_score test:   ' + str(sklearn.metrics.recall_score(y_test, y_test_pred, average=\"weighted\")))\n",
    "\n",
    "print('f1_score train:  ' + str(sklearn.metrics.f1_score(y_train, y_train_pred, average=\"weighted\")))\n",
    "print('f1_score test:   ' + str(sklearn.metrics.f1_score(y_test, y_test_pred, average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtrain = xgb.DMatrix(x_train, label=np.array(y_train))\n",
    "dtest = xgb.DMatrix(x_test, label=np.array(y_test))\n",
    "param_grid = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5]\n",
    "        }\n",
    "xgb_m = XGBClassifier()\n",
    "grid = GridSearchCV(xgb_m,param_grid, cv=5, verbose=50,refit = True,n_jobs=-1) \n",
    "grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_\n",
    "bst =  XGBClassifier(colsample_bytree= 0.6,\n",
    "                     gamma= 5,\n",
    "                     max_depth= 3,\n",
    "                     min_child_weight= 1,\n",
    "                     subsample= 1.0)\n",
    "bst.fit(x_train,y_train)\n",
    "y_train_pred = bst.predict(x_train)\n",
    "y_test_pred = bst.predict(x_test)\n",
    "\n",
    "print('accuracy train:  ' + str(sklearn.metrics.accuracy_score(y_train, y_train_pred)))\n",
    "print('cf_matrix on train:    ' + str(sklearn.metrics.confusion_matrix(y_train, y_train_pred)))\n",
    "\n",
    "print('accuracy test:   ' + str(sklearn.metrics.accuracy_score(y_test, y_test_pred)))\n",
    "print('cf_matrix: on test:     ' + str(sklearn.metrics.confusion_matrix(y_test, y_test_pred)))\n",
    "\n",
    "print('precision_score train:  ' + str(sklearn.metrics.precision_score(y_train, y_train_pred, average=\"weighted\")))\n",
    "print('precision_score test:   ' + str(sklearn.metrics.precision_score(y_test, y_test_pred, average=\"weighted\")))\n",
    "\n",
    "print('recall_score train:  ' + str(sklearn.metrics.recall_score(y_train, y_train_pred, average=\"weighted\")))\n",
    "print('recall_score test:   ' + str(sklearn.metrics.recall_score(y_test, y_test_pred, average=\"weighted\")))\n",
    "\n",
    "print('f1_score train:  ' + str(sklearn.metrics.f1_score(y_train, y_train_pred, average=\"weighted\")))\n",
    "print('f1_score test:   ' + str(sklearn.metrics.f1_score(y_test, y_test_pred, average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting ROC Curve for ERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "\n",
    "skplt.metrics.plot_roc_curve(y_test, ERT.predict_proba(x_test),ax=ax)\n",
    "#plt.ylim(0, 0.5)\n",
    "#gradient_image(ax, direction=0, extent=(0, 1, 0, 1), transform=ax.transAxes,cmap=plt.cm.Greens, cmap_range=(0.2, 0))\n",
    "\n",
    "    \n",
    "plt.ylabel('True Positive Rate',fontname='Times New Roman',fontweight=\"bold\")\n",
    "plt.xlabel('False Positive Rate',fontname='Times New Roman',fontweight=\"bold\")\n",
    "plt.xticks(fontname='Times New Roman')\n",
    "plt.yticks(fontname='Times New Roman')\n",
    "L = ax.legend()\n",
    "plt.setp(L.texts, family='Times New Roman')\n",
    "#plt.legend(['Accuracy'])\n",
    "plt.title('ROC Curves',fontname='Times New Roman')\n",
    "\n",
    "plt.grid(True,which='major',axis='y')\n",
    "plt.savefig('figs/ROC Curves.png',dpi=350,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr=ERT\n",
    "check=pd.read_csv('check2.csv',encoding='utf-8-sig')\n",
    "\n",
    "cols_df=pd.DataFrame(x_train.columns,columns=['varb'])\n",
    "cols_df['def']=check['def']\n",
    "cols_df.dropna(inplace=True)\n",
    "cols_df['importances_gbr'] = ERT.feature_importances_\n",
    "\n",
    "z = [ ]\n",
    "\n",
    "for item in [t for t in x_train.columns if t!='y']:\n",
    "    z.append(get_display(arabic_reshaper.reshape(item)))\n",
    "cols_df['fd']=z\n",
    "cols_df['hhh']=['feat'+i for i in np.arange(1,len([t for t in x_train.columns if t!='y'])+1).astype(str)]\n",
    "tbp=cols_df.sort_values(by=['importances_gbr'],ascending=False).head(n=10).reset_index(drop=True)\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "graph=sn.barplot(x=\"importances_gbr\", y=\"def\", data=cols_df.sort_values('importances_gbr', ascending=False), palette=\"viridis\")\n",
    "for  p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    ax.text(width +0.01,\n",
    "            p.get_y()+p.get_height()/2. + 0.2,\n",
    "            '{:1.4f}'.format(width),\n",
    "            ha=\"center\")\n",
    "plt.title(get_display(arabic_reshaper.reshape('Importance of Features')))\n",
    "plt.ylabel('Feature',fontname='Times New Roman',fontweight=\"bold\", fontsize=12)\n",
    "plt.xlabel('Importance (0-1)',fontname='Times New Roman',fontweight=\"bold\", fontsize=12)\n",
    "#plt.savefig('feature_importance.png',dpi=150,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn pdps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_df.loc[1,'def']='motorcycle involved'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in np.arange(0,len(x_train.columns),1):\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(6, 4))\n",
    "    feat_id=i\n",
    "    yy1=partial_dependence(ERT, x_train, features=feat_id)[0][0] #class\n",
    "    xx=partial_dependence(ERT, x_train, features=feat_id)[1][0] \n",
    "    ax1.plot(xx,yy1,label='PDO',linestyle='-.',linewidth=2)\n",
    "\n",
    "    yy2=partial_dependence(ERT, x_train, features=feat_id)[0][1] #class\n",
    "    ax1.plot(xx,yy2,label='Injury',linestyle='--',linewidth=2)\n",
    "\n",
    "    yy3=partial_dependence(ERT, x_train, features=feat_id)[0][2] #class\n",
    "    kamine=min([min(yy1),min(yy2),min(yy3)])\n",
    "    bishine=max([max(yy1),max(yy2),max(yy3)])+0.1\n",
    "\n",
    "    onvan=['PDO: '+'min='+str(round(min(yy1),3))+', max='+str(round(max(yy1),3)),\n",
    "                       'Injury: '+'min='+str(round(min(yy2),3))+', max='+str(round(max(yy2),3)),\n",
    "                       'Fatal: '+'min='+str(round(min(yy3),3))+', max='+str(round(max(yy3),3))] \n",
    "\n",
    "    ax1.plot(xx,yy3,label='Fatal',linestyle='dotted',linewidth=2)\n",
    "    ax1.legend(labels=onvan,fontsize=6,loc='best')\n",
    "    ax1.set_xlabel(xlabel=cols_df['def'][feat_id])\n",
    "    ax1.set_ylabel(ylabel=\"Probability\")\n",
    "    \n",
    "\n",
    "    plt.savefig('figs/pdp/'+cols_df['def'][feat_id].split('(')[0]+'.png',dpi=350,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ice plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#PDP Plot\n",
    "#ICE Plot\n",
    "#fig, pdp_ax = plt.subplots(figsize=(6,5))\n",
    "x_train.columns=cols_df['def']\n",
    "for s,q in zip([0,1,2],['PDO class','Injury class','Fatal class']):\n",
    "    \n",
    "    for  ix,label in (zip(cols_df['varb'],cols_df['def'])):\n",
    "        pdp_age = pdp.pdp_isolate(model=ERT, dataset=x_train, model_features=x_train.columns, feature=label)\n",
    "        fig, axes=pdp.pdp_plot(pdp_age, label, plot_lines=True, center=True,which_classes=[s],\n",
    "                              plot_params = {\n",
    "                    'title': 'ICE for feature \"%s\"' % label,'title_fontsize': 10,\n",
    "                    'subtitle_fontsize': 8,\n",
    "                    'font_family': 'Times New Roman','line_cmap': 'Blues'})\n",
    "        fig = matplotlib.pyplot.gcf()\n",
    "        fig.set_size_inches(6, 5)\n",
    "        plt.xlabel(q)\n",
    "        plt.ylabel('Change in Probability')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('figs/ice/centered/'+label.split('(')[0]+'_'+q+'_centered.png',dpi=350,bbox_inches='tight')\n",
    "#fig, ax = plt.subplots(figsize=(12,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction Partial Dependance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "l=cols_df.sort_values(by='importances_gbr',ascending=False).head(n=4)['def']\n",
    "m=cols_df.tail(n=3)['def']\n",
    "pairs=[(x,y)for x in l for y in m]\n",
    "\n",
    "p=cols_df.sort_values(by='importances_gbr',ascending=False).head(n=4)['varb']\n",
    "q=cols_df.tail(n=3)['varb']\n",
    "feds=[(x,y)for x in p for y in q]\n",
    "\n",
    "# Override to fix matplotlib issue\n",
    "def _pdp_contour_plot_override(X, Y, pdp_mx, inter_ax, cmap, norm, inter_fill_alpha, fontsize, plot_params):\n",
    "    contour_color = plot_params.get('contour_color', 'white')\n",
    "    level = np.min([X.shape[0], X.shape[1]])\n",
    "    c1 = inter_ax.contourf(X, Y, pdp_mx, N=level, origin='lower', cmap=cmap, norm=norm, alpha=inter_fill_alpha)\n",
    "    c2 = inter_ax.contour(c1, levels=c1.levels, colors=contour_color, origin='lower')\n",
    "    inter_ax.clabel(c2, fontsize=fontsize, inline=1)\n",
    "    inter_ax.set_aspect('auto')\n",
    "    return c1\n",
    "#prs = Presentation()\n",
    "pdp_plot_utils._pdp_contour_plot = _pdp_contour_plot_override\n",
    "interact_pdps={}\n",
    "for pair, fed in zip(pairs,feds):\n",
    "\n",
    "    pdp_inter = pdp.pdp_interact(model=gbr, dataset=x_train, model_features=x_train.columns, features=pair)\n",
    "    interact_pdps[pair]=pdp_inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=dict(zip([0,1,2],['PDO','injury','fatal']))\n",
    "\n",
    "for pair, fed in zip(pairs,feds):\n",
    "    for rade in [0,1,2]:\n",
    "        df_pdp=interact_pdps[pair][rade].pdp\n",
    "        zz=df_pdp.iloc[:,2]\n",
    "        xx=df_pdp.iloc[:,0]\n",
    "        yy=df_pdp.iloc[:,1]\n",
    "        fig = go.Figure(data =go.Contour(z=zz, x=xx, y=yy,colorscale='viridis', contours=dict(coloring ='heatmap',showlabels = True)))\n",
    "\n",
    "        fig.update_layout(autosize=True,width=600,height=800,margin=dict(l=20,r=20,b=20,t=20,pad=20),)\n",
    "        fig.update_xaxes(title=pair[0],title_font_family=\"Times New Roman\",title_font_size=20)\n",
    "        fig.update_yaxes(title=pair[1],title_font_family=\"Times New Roman\",title_font_size=20)\n",
    "        fig.update_layout(title={'text': \"<b>Interaction PDP:\"+classes[rade]+'_class','y':1,'x':0.5,'xanchor': 'center'\n",
    "                                 ,'yanchor': 'top','font_family':\"Times New Roman\",'font_size':24})\n",
    "        fig.write_image(\"figs/interactive_pdp/\"+classes[rade]+'_'+pair[0]+'_'+pair[1].split('(')[0]+\".png\")\n",
    "#\"interactive_pdp/pdpinteract_\"+pairs[0][0]+'_'+pairs[0][1]+"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
